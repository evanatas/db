# Аналитика медицинских изображений COVID-19 на стеке Hadoop / Spark

Итоговый проект по дисциплине **«Базы данных для компьютерного зрения»**.

Цель проекта — показать полный цикл работы с данными для эпидемиологического мониторинга COVID-19 на основе рентген-снимков грудной клетки:

- подготовка и очистка метаданных;
- организация хранения изображений и метаданных в структуре, совместимой с HDFS;
- загрузка и разбор данных в Apache Spark;
- SQL-аналитика (включая оконные функции);
- пример оптимизации хранения (партиционирование);
- пример обработки и простой ML-задачи в PySpark.

В качестве исходных данных используется открытый датасет  
**COVID-19 Chest X-Ray Dataset**: <https://github.com/ieee8023/covid-chestxray-dataset>.

---

## 1. Технологии

- **Python 3.x**
- **Jupyter Notebook**
- **Pandas** — предобработка и базовая аналитика;
- **PySpark / Apache Spark** — SQL-аналитика, обработка, ML;
- **HDFS-подобная структура хранения** — каталог `covid_dataset/` эмулирует структуру HDFS  
  (при наличии кластера может быть перенесён в настоящий HDFS).

---

## 2. Датасет и метаданные

Оригинальный репозиторий с данными:  
<https://github.com/ieee8023/covid-chestxray-dataset>

В работе используются:

- **изображения** — рентген-снимки (`images/*.png`, `*.jpg`) пациентов с COVID-19, пневмонией и другими патологиями;
- **метаданные** — файл `metadata.csv`, ключевые поля:
  - `patientid` — идентификатор пациента;
  - `age` — возраст;
  - `sex` — пол;
  - `finding` — исходный диагноз;
  - `view` — проекция снимка;
  - `date` — дата исследования.

После предобработки формируются файлы:

- `covid_dataset/metadata/metadata_cleaned.csv`
- `covid_dataset/metadata/metadata.parquet`

---

## 3. Структура проекта

```text
.
├── 01_preprocess_metadata.ipynb    # предобработка метаданных (Pandas)
├── 02_upload_to_hdfs.ipynb         # подготовка HDFS-подобной структуры и "загрузка" данных
├── 03_spark_analysis.ipynb         # аналитика и обработка в Spark (есть fallback на Pandas)
├── covid_dataset/
│   ├── images/
│   │   ├── covid/                  # снимки пациентов с COVID-19 (если загружены)
│   │   ├── pneumonia/              # снимки с пневмонией
│   │   ├── normal/                 # нормальные снимки
│   │   └── other/                  # прочие случаи
│   ├── metadata/
│   │   ├── metadata_cleaned.csv    # очищенный CSV с метаданными
│   │   └── metadata.parquet        # та же таблица в формате Parquet
│   └── processed/                  # результаты обработки / партиционированные данные
└── README.md

## 4. Ноутбуки

### 4.1. `01_preprocess_metadata.ipynb` — предобработка метаданных

**Задача:** очистка и подготовка метаданных к дальнейшему анализу.

**Основные шаги:**

- загрузка исходного файла `metadata.csv`;
- анализ качества данных:
  - пропуски в возрастах и поле;
  - возможные дубликаты;
  - разнородные диагнозы в поле `finding`;
- очистка:
  - удаление дубликатов;
  - заполнение пропусков в возрасте (например, случайными значениями из диапазона 20–80 лет);
  - приведение пола к единому виду (`M` / `F`);
- создание новых признаков:
  - `age_filled` — очищенный возраст;
  - `age_group` — возрастные группы (`child`, `adult`, `senior`);
  - `finding_unified` — укрупнённые диагнозы (`COVID19`, `PNEUMONIA`, `NORMAL`, `OTHER`);
- сохранение результатов в файлы:
  - `covid_dataset/metadata/metadata_cleaned.csv`;
  - `covid_dataset/metadata/metadata.parquet`.

---

### 4.2. `02_upload_to_hdfs.ipynb` — подготовка структуры хранения

**Задача:** развернуть HDFS-подобную структуру хранения и «загрузить» в неё данные.

**Основные шаги:**

- определение корневого каталога `covid_dataset/` и подкаталогов:
  - `metadata/` — очищенные метаданные в формате CSV и Parquet;
  - `images/` с подпапками `covid/`, `pneumonia/`, `normal/`, `other/`;
  - `processed/` — будущие результаты обработки;
- вспомогательные функции:
  - `make_dir(path)` — создание каталогов:
    - либо через `hdfs dfs -mkdir -p` (если доступен HDFS);
    - либо как обычных локальных папок (эмуляция HDFS);
  - `put_file(src, dst_dir)` — копирование файлов:
    - либо через `hdfs dfs -put`;
    - либо через стандартный `shutil.copy2` в локальную структуру;
- копирование очищенных метаданных (`metadata_cleaned.csv`, `metadata.parquet`)
  в каталог `covid_dataset/metadata/`;
- (опционально) раскладка изображений по подпапкам `covid/`, `pneumonia/`, `normal/`, `other`
  в зависимости от признака `finding_unified`.

---

### 4.3. `03_spark_analysis.ipynb` — аналитика и ML в Spark

**Задача:** выполнить аналитические запросы и простой ML-анализ в Apache Spark.

**Основные шаги:**

- попытка создать `SparkSession` в режиме `local[*]`:
  - при успешном старте используется Spark-режим;
  - при ошибке ноутбук переключается в резервный режим на Pandas;
- загрузка `covid_dataset/metadata/metadata.parquet`:
  - в `df_spark` (Spark DataFrame) при работе со Spark;
  - либо в `df_pd` (Pandas DataFrame) в резервном режиме;
- SQL-аналитика в Spark:
  - распределение укрупнённых диагнозов (`finding_unified`);
  - таблица «возрастная группа × диагноз»;
  - пример оконной функции: ранжирование пациентов по возрасту внутри диагноза
    (`RANK() OVER (PARTITION BY finding_unified ORDER BY age_filled DESC)`);
- оптимизация хранения:
  - запись данных в формате Parquet с партиционированием по
    `finding_unified` и `age_group` в каталог `covid_dataset/processed/...`;
- пример обработки в PySpark:
  - UDF для повторной категоризации возраста в возрастные группы;
- пример ML-задачи (Spark ML):
  - подготовка признаков (возраст, пол);
  - кластеризация пациентов методом **KMeans** на основе возраста и пола;
  - интерпретация центров кластеров (типичные группы пациентов).

---
## 5. Запуск проекта

1. Установить зависимости (минимальный набор):

   ```bash
   pip install pandas pyarrow pyspark jupyter
2. При использовании Spark нужна установленная **Java (JDK 11 или 17)**  
   и настроенная переменная окружения `JAVA_HOME`.

3. Запустить Jupyter Notebook и последовательно выполнить ноутбуки:

   1. `01_preprocess_metadata.ipynb` — предобработка и сохранение метаданных  
      в `covid_dataset/metadata/metadata_cleaned.csv` и `metadata.parquet`;
   2. `02_upload_to_hdfs.ipynb` — создание структуры каталога `covid_dataset/`  
      и «загрузка» туда очищенных метаданных и (опционально) изображений;
   3. `03_spark_analysis.ipynb` — аналитика в Spark (SQL-запросы, оконные функции,  
      партиционирование и пример ML-задачи).

4. При наличии реального HDFS-кластера каталог `covid_dataset/`  
   можно перенести в HDFS, а функции из `02_upload_to_hdfs.ipynb`  
   адаптировать к использованию команд `hdfs dfs`.

## 6. Результаты и возможные улучшения

**Что реализовано:**

- построен полный цикл работы с данными:
  - предобработка метаданных (очистка, заполнение пропусков, новые признаки);
  - организация хранения в HDFS-подобной структуре `covid_dataset/`;
  - аналитика в Apache Spark (Spark SQL, оконные функции, партиционирование);
  - пример использования Spark ML (кластеризация KMeans по возрасту и полу);
- подготовлены ноутбуки, которые можно запускать как в локальном окружении,
  так и в среде с полноценным кластером Hadoop / Spark.

**Ограничения:**

- неполнота исходных метаданных (часть записей без возраста и пола);
- размер датасета ограничен, поэтому выводы носят иллюстративный характер;
- в проекте анализируются только метаданные, без обучения сложной CV-модели
  по самим изображениям.

**Возможные направления развития:**

- подключить настоящий HDFS-кластер и Hive для выполнения запросов в распределённой среде;
- добавить дашборды (например, на базе `matplotlib`/`seaborn` или BI-инструментов)
  для мониторинга показателей по времени;
- интегрировать модель компьютерного зрения для автоматической классификации снимков
  и сравнения предсказаний с диагнозами в метаданных;
- улучшить этап предобработки (более аккуратное заполнение возрастов, учёт нескольких
  снимков одного пациента, работа с временными рядами по датам исследований).
