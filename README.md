# Аналитика медицинских изображений COVID-19 на стеке Hadoop / Spark

Итоговый проект по дисциплине **«Базы данных для компьютерного зрения»**.

Цель проекта — показать полный цикл работы с данными для эпидемиологического мониторинга COVID-19 на основе рентген-снимков грудной клетки:

- подготовка и очистка метаданных;
- организация хранения изображений и метаданных в структуре, совместимой с HDFS;
- загрузка и разбор данных в Apache Spark;
- SQL-аналитика (включая оконные функции);
- пример оптимизации хранения (партиционирование);
- пример PySpark-обработки и простой ML-задачи.

В качестве примера используется открытый датасет  
**COVID-19 Chest X-Ray Dataset**: <https://github.com/ieee8023/covid-chestxray-dataset>.

---

## 1. Технологии

- **Python 3.x**
- **Jupyter Notebook**
- **Pandas** — предобработка и локальная аналитика;
- **PySpark / Apache Spark** — SQL-аналитика, обработка, ML;
- **HDFS-подобная структура хранения** — каталог `covid_dataset/` эмулирует структуру HDFS  
  (при наличии реального кластера может быть заменён на настоящий HDFS).

---

## 2. Датасет и метаданные

Оригинальный репозиторий с данными:  
<https://github.com/ieee8023/covid-chestxray-dataset>

В работе используются:

- **изображения** — рентген-снимки (`images/*.png`, `*.jpg`) пациентов с COVID-19, пневмонией и др. патологиями;
- **метаданные** — файл `metadata.csv`, содержащий:
  - `patientid` — идентификатор пациента;
  - `age` — возраст;
  - `sex` — пол;
  - `finding` — исходный диагноз;
  - `view` — проекция снимка;
  - `date` — дата исследования.

После предобработки формируются файлы:

- `covid_dataset/metadata/metadata_cleaned.csv`
- `covid_dataset/metadata/metadata.parquet`

---

## 3. Структура проекта

```text
.
├── 01_preprocess_metadata.ipynb    # предобработка метаданных (Pandas)
├── 02_upload_to_hdfs.ipynb         # подготовка HDFS-подобной структуры и "загрузка" данных
├── 03_spark_analysis.ipynb         # аналитика и обработка в Spark (есть fallback на Pandas)
├── covid_dataset/
│   ├── images/
│   │   ├── covid/                  # снимки пациентов с COVID-19 (если загружены)
│   │   ├── pneumonia/              # снимки с пневмонией
│   │   ├── normal/                 # нормальные снимки
│   │   └── other/                  # прочие случаи
│   ├── metadata/
│   │   ├── metadata_cleaned.csv    # очищенный CSV с метаданными
│   │   └── metadata.parquet        # та же таблица в формате Parquet
│   └── processed/                  # результаты обработки / партиционированные данные
└── README.md
4. Описание ноутбуков
4.1. 01_preprocess_metadata.ipynb

Задача: очистка и подготовка метаданных.

Основные шаги:

Загрузка исходного metadata.csv.

Анализ качества данных (пропуски, дубли, распределение диагнозов).

Очистка:

удаление дубликатов;

заполнение пропусков в возрасте;

приведение пола к единому виду.

Создание новых признаков:

age_filled — очищенный возраст;

age_group — возрастные группы (child, adult, senior);

finding_unified — укрупнённые диагнозы (COVID19, PNEUMONIA, NORMAL, OTHER).

Сохранение результатов в metadata_cleaned.csv и metadata.parquet.

4.2. 02_upload_to_hdfs.ipynb

Задача: подготовка структуры хранения, совместимой с HDFS, и логика загрузки данных.

Основные шаги:

Определение корневого каталога covid_dataset/ и подкаталогов:

metadata/ — очищенные метаданные (CSV и Parquet);

images/ и подпапки covid/, pneumonia/, normal/, other/;

processed/ — будущие результаты обработки.

Функции:

make_dir(...) — создание каталогов (реальный HDFS либо локальная файловая система);

put_file(...) — копирование файлов (через hdfs dfs -put или обычный copy).

Копирование очищенных метаданных в covid_dataset/metadata/.

(Опционально) Раскладка файлов изображений по папкам в зависимости от finding_unified.

4.3. 03_spark_analysis.ipynb

Задача: аналитика и обработка данных в Apache Spark.

Основные шаги:

Попытка создать SparkSession в режиме local[*].
При ошибке (например, проблемы с Java) ноутбук переключается в резервный режим анализа на Pandas.

Загрузка covid_dataset/metadata/metadata.parquet:

в df_spark (Spark DataFrame) при успешном старте Spark;

или в df_pd (Pandas DataFrame) при резервном режиме.

SQL-аналитика:

распределение укрупнённых диагнозов;

таблица «возрастная группа × диагноз»;

пример оконной функции: ранжирование пациентов по возрасту внутри диагноза.

Оптимизация хранения:

запись набора в формате Parquet с партиционированием по finding_unified и age_group
в каталог covid_dataset/processed/....

Пример обработки в PySpark:

UDF для повторной категоризации возраста.

Пример ML-задачи (Spark ML):

подготовка признаков (возраст, пол);

кластеризация пациентов по возрасту и полу с помощью KMeans.

5. Запуск

Установить зависимости:

pip install pandas pyarrow pyspark jupyter


При работе со Spark требуется установленная Java (JDK 11/17).
В локальном режиме ноутбуки запускаются в Jupyter последовательно:

01_preprocess_metadata.ipynb

02_upload_to_hdfs.ipynb

03_spark_analysis.ipynb

При наличии настроенного кластера Hadoop/Spark covid_dataset/ может быть перенесён в настоящий HDFS, а команды в ноутбуке 02_upload_to_hdfs.ipynb легко адаптируются под hdfs dfs.
