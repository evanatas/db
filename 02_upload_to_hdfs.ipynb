{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 02_upload_to_hdfs.ipynb\n",
    "\n",
    "Загрузка очищенных данных и изображений в HDFS для проекта анализа COVID-19 рентген-снимков.\n",
    "\n",
    "В этом ноутбуке мы:\n",
    "- загружаем файл `metadata_cleaned.csv`, подготовленный на предыдущем шаге;\n",
    "- настраиваем пути к локальным данным и к HDFS;\n",
    "- создаём в HDFS структуру каталогов `/covid_dataset/...`;\n",
    "- загружаем в HDFS очищенные метаданные (CSV и Parquet);\n",
    "- загружаем изображения в HDFS, раскладывая их по подкаталогам в зависимости от диагноза (COVID19 / PNEUMONIA / NORMAL / OTHER).\n",
    "\n",
    "**Предположения:**\n",
    "- ноутбук запускается в окружении, где доступна команда `hdfs` (например, внутри Docker-кластера с Hadoop + Spark);\n",
    "- рядом с этим ноутбуком лежат файлы `metadata_cleaned.csv` и `metadata.parquet`;\n",
    "- папка с изображениями `images/` находится по указанному ниже пути (его можно изменить под свою структуру).\n"
   ],
   "id": "fc0fb7aefd0715c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:28:47.656589Z",
     "start_time": "2025-12-11T17:28:47.647588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os                   # Модуль для работы с файловой системой (пути к файлам и папкам)\n",
    "import subprocess           # Модуль для запуска системных команд (будем вызывать hdfs dfs)\n",
    "import pandas as pd         # Библиотека для работы с табличными данными (DataFrame)\n",
    "import shutil  # модуль для работы с путями к бинарникам (ищет, где лежит исполняемый файл)\n",
    "\n",
    "# Проверяем, доступна ли в системе команда 'hdfs'\n",
    "HDFS_BIN = shutil.which(\"hdfs\")  # вернёт путь к бинарнику 'hdfs' или None, если его нет\n",
    "\n",
    "if HDFS_BIN is None:\n",
    "    print(\"⚠️ Команда 'hdfs' не найдена в системе. Все HDFS-команды будут только печататься (без реального выполнения).\")\n",
    "else:\n",
    "    print(\"Найден бинарник hdfs по пути:\", HDFS_BIN)\n"
   ],
   "id": "464f68ec62e16463",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Команда 'hdfs' не найдена в системе. Все HDFS-команды будут только печататься (без реального выполнения).\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:28:58.432608Z",
     "start_time": "2025-12-11T17:28:58.430264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Локальные пути (относительно папки, где лежит этот ноутбук)\n",
    "LOCAL_METADATA_CSV_PATH = \"metadata_cleaned.csv\"   # путь к очищенному CSV с метаданными\n",
    "LOCAL_METADATA_PARQUET_PATH = \"metadata.parquet\"   # путь к очищенному Parquet-файлу\n",
    "LOCAL_IMAGES_DIR = \"images\"                        # папка с изображениями рентген-снимков\n",
    "\n",
    "# Корневой путь в HDFS, где будет храниться наш датасет\n",
    "HDFS_BASE_DIR = \"/covid_dataset\"                   # основной каталог для проекта в HDFS\n",
    "\n",
    "# Подкаталоги внутри HDFS\n",
    "HDFS_METADATA_DIR = f\"{HDFS_BASE_DIR}/metadata\"    # сюда положим метаданные (CSV и Parquet)\n",
    "HDFS_IMAGES_DIR = f\"{HDFS_BASE_DIR}/images\"        # сюда положим изображения по категориям\n",
    "HDFS_PROCESSED_DIR = f\"{HDFS_BASE_DIR}/processed\"  # сюда будем складывать будущие производные данные (если понадобятся)\n"
   ],
   "id": "25ba626e9c3ccde8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:28:58.888437Z",
     "start_time": "2025-12-11T17:28:58.877224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Загружаем очищенный CSV-файл с метаданными\n",
    "metadata_clean = pd.read_csv(LOCAL_METADATA_CSV_PATH)\n",
    "\n",
    "# Показываем первые строки таблицы, чтобы убедиться, что всё читается корректно\n",
    "metadata_clean.head()\n"
   ],
   "id": "798f91ca828bff31",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  patientid                                           filename   age  \\\n",
       "0         2  auntminnie-a-2020_01_28_23_51_6665_2020_01_28_...  65.0   \n",
       "1         2  auntminnie-b-2020_01_28_23_51_6665_2020_01_28_...  65.0   \n",
       "2         2  auntminnie-c-2020_01_28_23_51_6665_2020_01_28_...  65.0   \n",
       "3         2  auntminnie-d-2020_01_28_23_51_6665_2020_01_28_...  65.0   \n",
       "4         4                              nejmc2001573_f1a.jpeg  52.0   \n",
       "\n",
       "   age_filled age_group sex view                   finding finding_unified  \\\n",
       "0        65.0    senior   M   PA  Pneumonia/Viral/COVID-19         COVID19   \n",
       "1        65.0    senior   M   PA  Pneumonia/Viral/COVID-19         COVID19   \n",
       "2        65.0    senior   M   PA  Pneumonia/Viral/COVID-19         COVID19   \n",
       "3        65.0    senior   M   PA  Pneumonia/Viral/COVID-19         COVID19   \n",
       "4        52.0     adult   F   PA  Pneumonia/Viral/COVID-19         COVID19   \n",
       "\n",
       "               date  \n",
       "0  January 22, 2020  \n",
       "1  January 25, 2020  \n",
       "2  January 27, 2020  \n",
       "3  January 28, 2020  \n",
       "4  January 25, 2020  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientid</th>\n",
       "      <th>filename</th>\n",
       "      <th>age</th>\n",
       "      <th>age_filled</th>\n",
       "      <th>age_group</th>\n",
       "      <th>sex</th>\n",
       "      <th>view</th>\n",
       "      <th>finding</th>\n",
       "      <th>finding_unified</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>auntminnie-a-2020_01_28_23_51_6665_2020_01_28_...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>senior</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>Pneumonia/Viral/COVID-19</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>January 22, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>auntminnie-b-2020_01_28_23_51_6665_2020_01_28_...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>senior</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>Pneumonia/Viral/COVID-19</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>January 25, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>auntminnie-c-2020_01_28_23_51_6665_2020_01_28_...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>senior</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>Pneumonia/Viral/COVID-19</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>January 27, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>auntminnie-d-2020_01_28_23_51_6665_2020_01_28_...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>senior</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>Pneumonia/Viral/COVID-19</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>January 28, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>nejmc2001573_f1a.jpeg</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>adult</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>Pneumonia/Viral/COVID-19</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>January 25, 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:28:59.265116Z",
     "start_time": "2025-12-11T17:28:59.262076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_hdfs_command(command_list):\n",
    "    \"\"\"\n",
    "    Вспомогательная функция для запуска команд hdfs dfs через subprocess.\n",
    "    \n",
    "    command_list: список строк, например [\"hdfs\", \"dfs\", \"-ls\", \"/\"].\n",
    "    \"\"\"\n",
    "    print(\"Выполняем команду:\", \" \".join(command_list))  # печатаем команду, чтобы видеть, что происходит\n",
    "\n",
    "    result = subprocess.run(          # запускаем внешнюю команду\n",
    "        command_list,                 # список аргументов, например [\"hdfs\", \"dfs\", \"-mkdir\", \"-p\", \"/covid_dataset\"]\n",
    "        stdout=subprocess.PIPE,       # перенаправляем стандартный вывод (stdout), чтобы потом прочитать его в Python\n",
    "        stderr=subprocess.PIPE,       # перенаправляем стандартный вывод ошибок (stderr)\n",
    "        text=True                     # говорим, что нужно работать со строками, а не с байтами (Python сам декодирует)\n",
    "    )\n",
    "\n",
    "    if result.stdout:                 # если команда что-то вывела в stdout\n",
    "        print(\"STDOUT:\")              # печатаем заголовок\n",
    "        print(result.stdout)          # печатаем сам вывод\n",
    "\n",
    "    if result.stderr:                 # если в stderr есть что-то (часто там варнинги или ошибки)\n",
    "        print(\"STDERR:\")              # печатаем заголовок\n",
    "        print(result.stderr)          # печатаем текст ошибок/варнингов\n",
    "\n",
    "    if result.returncode != 0:        # если код возврата команды не равен нулю, значит произошла ошибка\n",
    "        print(\"Команда завершилась с ошибкой, код:\", result.returncode)  # сообщаем об этом\n"
   ],
   "id": "d5a1f54fac070bab",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:41:59.725188Z",
     "start_time": "2025-12-11T17:41:59.707020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shutil  # модуль для копирования файлов и поиска бинарников\n",
    "\n",
    "# Флаг: работаем ли мы с реальным HDFS (True) или эмулируем его локальными папками (False)\n",
    "USE_HDFS = False\n",
    "# Сейчас у тебя на ноуте нет команды hdfs, поэтому оставляем False.\n",
    "# Если потом будешь запускать это в Docker-кластере с Hadoop — можно поставить True.\n",
    "\n",
    "# Проверяем, доступна ли команда 'hdfs' в системе (на будущее)\n",
    "HDFS_BIN = shutil.which(\"hdfs\")  # вернёт строку с путём к бинарнику 'hdfs' или None, если его нет\n",
    "\n",
    "if USE_HDFS:\n",
    "    # Если мы хотим реальный HDFS\n",
    "    if HDFS_BIN is None:\n",
    "        print(\"⚠️ USE_HDFS=True, но команда 'hdfs' не найдена в PATH. Команды hdfs dfs будут падать.\")\n",
    "    else:\n",
    "        print(\"Режим: реальный HDFS. Найден бинарник hdfs по пути:\", HDFS_BIN)\n",
    "else:\n",
    "    # Если мы работаем в локальном режиме (эмуляция HDFS обычными папками)\n",
    "    print(\"Режим: эмуляция HDFS через локальные папки.\")\n",
    "\n",
    "\n",
    "def hdfs_path_to_local(hdfs_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Преобразует HDFS-путь вида '/covid_dataset/metadata'\n",
    "    в локальный путь 'covid_dataset/metadata', чтобы создавать такие же директории на диске.\n",
    "    \"\"\"\n",
    "    # Если путь начинается с / (корень), отбрасываем первый символ\n",
    "    if hdfs_path.startswith(\"/\"):\n",
    "        return hdfs_path[1:]\n",
    "    # Если слэша нет, возвращаем строку как есть\n",
    "    return hdfs_path\n",
    "\n",
    "\n",
    "def make_dir(hdfs_dir_path: str):\n",
    "    \"\"\"\n",
    "    Создаёт каталог:\n",
    "    - либо в реальном HDFS (если USE_HDFS=True и есть команда hdfs),\n",
    "    - либо как локальную папку (если USE_HDFS=False).\n",
    "    \"\"\"\n",
    "    if USE_HDFS and HDFS_BIN is not None:\n",
    "        # Реальный HDFS: строим команду hdfs dfs -mkdir -p <путь>\n",
    "        cmd = [\"hdfs\", \"dfs\", \"-mkdir\", \"-p\", hdfs_dir_path]\n",
    "        print(\"Создаём каталог в HDFS командой:\", \" \".join(cmd))\n",
    "        # Запускаем внешнюю команду\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        # Если есть вывод в stdout — печатаем\n",
    "        if result.stdout:\n",
    "            print(\"STDOUT:\")\n",
    "            print(result.stdout)\n",
    "        # Если есть вывод в stderr (ошибки/варнинги) — тоже печатаем\n",
    "        if result.stderr:\n",
    "            print(\"STDERR:\")\n",
    "            print(result.stderr)\n",
    "    else:\n",
    "        # Локальная эмуляция HDFS: создаём обычную папку в файловой системе\n",
    "        local_dir = hdfs_path_to_local(hdfs_dir_path)  # переводим \"/covid_dataset/...\" → \"covid_dataset/...\"\n",
    "        print(\"Создаём локальный каталог (эмуляция HDFS):\", local_dir)\n",
    "        os.makedirs(local_dir, exist_ok=True)          # создаём все промежуточные папки, если их ещё нет\n",
    "\n",
    "\n",
    "def put_file(local_src_path: str, hdfs_dst_dir: str):\n",
    "    \"\"\"\n",
    "    \"Кладёт\" файл:\n",
    "    - либо в реальный HDFS (через hdfs dfs -put),\n",
    "    - либо в локальную папку (копированием файла).\n",
    "    \"\"\"\n",
    "    if USE_HDFS and HDFS_BIN is not None:\n",
    "        # Реальный HDFS: команда hdfs dfs -put -f <local> <hdfs_dir>\n",
    "        cmd = [\"hdfs\", \"dfs\", \"-put\", \"-f\", local_src_path, hdfs_dst_dir]\n",
    "        print(\"Кладём файл в HDFS командой:\", \" \".join(cmd))\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        if result.stdout:\n",
    "            print(\"STDOUT:\")\n",
    "            print(result.stdout)\n",
    "        if result.stderr:\n",
    "            print(\"STDERR:\")\n",
    "            print(result.stderr)\n",
    "    else:\n",
    "        # Локальная эмуляция: просто копируем файл на диск в нужную папку\n",
    "        local_dst_dir = hdfs_path_to_local(hdfs_dst_dir)      # \"/covid_dataset/metadata\" → \"covid_dataset/metadata\"\n",
    "        os.makedirs(local_dst_dir, exist_ok=True)             # убеждаемся, что папка существует\n",
    "        dst_path = os.path.join(local_dst_dir, os.path.basename(local_src_path))  # полный путь назначения\n",
    "        print(f\"Копируем файл {local_src_path} → {dst_path} (эмуляция HDFS)\")\n",
    "        shutil.copy2(local_src_path, dst_path)                # копируем файл, сохраняя метаданные (время и т.п.)\n"
   ],
   "id": "2fed12d357fec209",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Режим: эмуляция HDFS через локальные папки.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:42:23.259359Z",
     "start_time": "2025-12-11T17:42:23.251510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Создаём корневой каталог /covid_dataset\n",
    "make_dir(HDFS_BASE_DIR)          # будет создан локальный каталог \"covid_dataset/\"\n",
    "\n",
    "# 2. Создаём подкаталог для метаданных /covid_dataset/metadata\n",
    "make_dir(HDFS_METADATA_DIR)      # локально будет \"covid_dataset/metadata/\"\n",
    "\n",
    "# 3. Создаём подкаталог для изображений /covid_dataset/images\n",
    "make_dir(HDFS_IMAGES_DIR)        # локально \"covid_dataset/images/\"\n",
    "\n",
    "# 4. Создаём подкаталог для обработанных данных /covid_dataset/processed\n",
    "make_dir(HDFS_PROCESSED_DIR)     # локально \"covid_dataset/processed/\"\n"
   ],
   "id": "45aed981ac4296e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создаём локальный каталог (эмуляция HDFS): covid_dataset\n",
      "Создаём локальный каталог (эмуляция HDFS): covid_dataset/metadata\n",
      "Создаём локальный каталог (эмуляция HDFS): covid_dataset/images\n",
      "Создаём локальный каталог (эмуляция HDFS): covid_dataset/processed\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:43:00.219175Z",
     "start_time": "2025-12-11T17:43:00.211573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Определяем пути к подкаталогам с изображениями по укрупнённым диагнозам\n",
    "HDFS_IMAGES_COVID_DIR = f\"{HDFS_IMAGES_DIR}/covid\"         # снимки пациентов с COVID-19\n",
    "HDFS_IMAGES_PNEUMONIA_DIR = f\"{HDFS_IMAGES_DIR}/pneumonia\" # снимки пациентов с пневмонией\n",
    "HDFS_IMAGES_NORMAL_DIR = f\"{HDFS_IMAGES_DIR}/normal\"       # нормальные снимки без патологии\n",
    "HDFS_IMAGES_OTHER_DIR = f\"{HDFS_IMAGES_DIR}/other\"         # прочие случаи\n",
    "\n",
    "# Создаём эти каталоги (в HDFS или локально — в зависимости от режима)\n",
    "make_dir(HDFS_IMAGES_COVID_DIR)        # /covid_dataset/images/covid  → локально covid_dataset/images/covid\n",
    "make_dir(HDFS_IMAGES_PNEUMONIA_DIR)    # /covid_dataset/images/pneumonia\n",
    "make_dir(HDFS_IMAGES_NORMAL_DIR)       # /covid_dataset/images/normal\n",
    "make_dir(HDFS_IMAGES_OTHER_DIR)        # /covid_dataset/images/other\n"
   ],
   "id": "89c305fcdffc535f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создаём локальный каталог (эмуляция HDFS): covid_dataset/images/covid\n",
      "Создаём локальный каталог (эмуляция HDFS): covid_dataset/images/pneumonia\n",
      "Создаём локальный каталог (эмуляция HDFS): covid_dataset/images/normal\n",
      "Создаём локальный каталог (эмуляция HDFS): covid_dataset/images/other\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:43:09.927456Z",
     "start_time": "2025-12-11T17:43:09.923249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Кладём очищенный CSV-файл с метаданными в \"HDFS\"-каталог metadata\n",
    "put_file(LOCAL_METADATA_CSV_PATH, HDFS_METADATA_DIR)        # metadata_cleaned.csv → covid_dataset/metadata/\n",
    "\n",
    "# Кладём Parquet-файл с метаданными туда же\n",
    "put_file(LOCAL_METADATA_PARQUET_PATH, HDFS_METADATA_DIR)    # metadata.parquet → covid_dataset/metadata/\n"
   ],
   "id": "91757babc38c6050",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Копируем файл metadata_cleaned.csv → covid_dataset/metadata/metadata_cleaned.csv (эмуляция HDFS)\n",
      "Копируем файл metadata.parquet → covid_dataset/metadata/metadata.parquet (эмуляция HDFS)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:43:20.474037Z",
     "start_time": "2025-12-11T17:43:20.462454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Словарь сопоставления укрупнённого диагноза и HDFS-папки с изображениями\n",
    "DIAGNOSIS_TO_HDFS_DIR = {\n",
    "    \"COVID19\": HDFS_IMAGES_COVID_DIR,        # COVID-случаи\n",
    "    \"PNEUMONIA\": HDFS_IMAGES_PNEUMONIA_DIR,  # пневмонии\n",
    "    \"NORMAL\": HDFS_IMAGES_NORMAL_DIR,        # нормальные снимки\n",
    "    \"OTHER\": HDFS_IMAGES_OTHER_DIR           # все остальные\n",
    "}\n",
    "\n",
    "# Смотрим распределение укрупнённых диагнозов\n",
    "print(\"Распределение укрупнённых диагнозов:\")\n",
    "print(metadata_clean[\"finding_unified\"].value_counts())  # сколько записей в каждой категории\n",
    "\n",
    "# Проверяем, есть ли пропуски в имени файла (filename)\n",
    "missing_filenames = metadata_clean[\"filename\"].isna().sum()  # считаем количество NaN в колонке filename\n",
    "print(\"Пропусков в колонке 'filename':\", missing_filenames)  # выводим результат\n"
   ],
   "id": "1e079b212fe4b9c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение укрупнённых диагнозов:\n",
      "finding_unified\n",
      "COVID19      584\n",
      "PNEUMONIA    242\n",
      "OTHER        102\n",
      "NORMAL        22\n",
      "Name: count, dtype: int64\n",
      "Пропусков в колонке 'filename': 0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:49:07.914036Z",
     "start_time": "2025-12-11T17:49:07.904710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Счётчик загруженных файлов по категориям (для статистики)\n",
    "uploaded_counts = {\n",
    "    \"covid\": 0,\n",
    "    \"pneumonia\": 0,\n",
    "    \"normal\": 0,\n",
    "    \"other\": 0\n",
    "}\n",
    "\n",
    "# Если папки с изображениями вообще нет — честно говорим об этом и не пытаемся ничего копировать\n",
    "if not os.path.isdir(LOCAL_IMAGES_DIR):\n",
    "    print(f\"⚠️ Папка с изображениями '{LOCAL_IMAGES_DIR}' не найдена.\")\n",
    "    print(\"   Предполагается, что здесь лежит папка с рентген-снимками датасета.\")\n",
    "    print(\"   Загрузку изображений в HDFS пропускаем, но структура каталогов уже создана.\")\n",
    "else:\n",
    "    # Если папка есть, пробуем реально разложить файлы по категориям\n",
    "    for index, row in metadata_clean.iterrows():\n",
    "        filename = row[\"filename\"]                  # имя файла изображения\n",
    "        diagnosis = row[\"finding_unified\"]          # укрупнённый диагноз\n",
    "\n",
    "        if pd.isna(filename):\n",
    "            print(f\"Строка {index}: пропущено, нет имени файла\")\n",
    "            continue\n",
    "\n",
    "        # Выбираем целевую папку по диагнозу\n",
    "        hdfs_target_dir = DIAGNOSIS_TO_HDFS_DIR.get(diagnosis, HDFS_IMAGES_OTHER_DIR)\n",
    "\n",
    "        # Склеиваем локальный путь\n",
    "        local_image_path = os.path.join(LOCAL_IMAGES_DIR, filename)\n",
    "\n",
    "        # Проверяем, что файл существует\n",
    "        if not os.path.exists(local_image_path):\n",
    "            print(f\"Строка {index}: файл {local_image_path} не найден, пропускаем\")\n",
    "            continue\n",
    "\n",
    "        # Кладём файл (в HDFS или локально — в зависимости от режима)\n",
    "        put_file(local_image_path, hdfs_target_dir)\n",
    "\n",
    "        # Обновляем счётчик\n",
    "        if hdfs_target_dir == HDFS_IMAGES_COVID_DIR:\n",
    "            uploaded_counts[\"covid\"] += 1\n",
    "        elif hdfs_target_dir == HDFS_IMAGES_PNEUMONIA_DIR:\n",
    "            uploaded_counts[\"pneumonia\"] += 1\n",
    "        elif hdfs_target_dir == HDFS_IMAGES_NORMAL_DIR:\n",
    "            uploaded_counts[\"normal\"] += 1\n",
    "        else:\n",
    "            uploaded_counts[\"other\"] += 1\n",
    "\n",
    "    print(\"Загрузка изображений завершена.\")\n",
    "    print(\"Статистика по загруженным файлам:\")\n",
    "    for category, count in uploaded_counts.items():\n",
    "        print(f\"{category}: {count}\")\n"
   ],
   "id": "828d4926b35db4f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Папка с изображениями 'images' не найдена.\n",
      "   Предполагается, что здесь лежит папка с рентген-снимками датасета.\n",
      "   Загрузку изображений в HDFS пропускаем, но структура каталогов уже создана.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4e8ba0f722907e2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "66420c5be1bcd0f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cae11ddf93320b5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8d3aae5b05b9c5a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dcf065dfc709d35e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "57df1eb56cbc9f38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d663265c85cc12b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "97aa70c34d0b0f9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "43ddfba467fc588b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0d875e3f7156c61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4bbe7f9799874299"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a4c98b49665d356c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3f9c3e86ff0ea945"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7c669a116ec9b171"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7c73658a668d600e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4e9e3647bb69fa4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd5d874a23881a6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "65439c2eee2a3a33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "92f58849884b5822"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2b59ba36e061e222"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7c8e4a5a991e5d73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b77a859b55a114a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "966926f8d1535726"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4d10d75093bab8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b24c8f6506e1bfa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6ba411ffd5abc26e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7fdf282a00776415"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3898fb96a3842c4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4e5fbfd80792f2b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e0c19191ad74187"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "84e1178fda281893"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "318742e4571f2cc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eaa920f02ada7f04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cfa259fa53ae2adf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
