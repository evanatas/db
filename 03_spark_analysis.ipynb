{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 03_spark_analysis.ipynb\n",
    "\n",
    "Аналитика и обработка данных в Apache Spark для проекта по COVID-19 Chest X-Ray Dataset.\n",
    "\n",
    "В этом ноутбуке мы:\n",
    "\n",
    "- создаём локальную Spark-сессию (`master = local[*]`, без кластера);\n",
    "- загружаем очищенные метаданные из файла `covid_dataset/metadata/metadata.parquet`;\n",
    "- регистрируем их как временную Spark SQL-вью `covid_metadata`;\n",
    "- выполняем несколько аналитических SQL-запросов:\n",
    "  - распределение диагнозов и их долей;\n",
    "  - распределение пациентов по возрастным группам и диагнозам;\n",
    "  - пример оконной функции: ранжирование пациентов по возрасту внутри диагноза;\n",
    "- показываем пример оптимизации хранения:\n",
    "  - запись данных в формате Parquet с партиционированием по `finding_unified` и `age_group`;\n",
    "- демонстрируем пример обработки в PySpark:\n",
    "  - использование UDF для повторной категоризации возраста;\n",
    "- показываем пример ML-задачи в Spark ML:\n",
    "  - кластеризация пациентов по возрасту и полу с помощью алгоритма K-Means;\n",
    "- сохраняем результаты аналитики в каталог `covid_dataset/processed/` для дальнейшей визуализации.\n",
    "\n",
    "> Для удобства локального тестирования при отсутствии рабочего Spark-окружения предусмотрен запасной режим анализа на Pandas, но основной код ноутбука использует API Apache Spark и готов к запуску в кластере / Docker-окружении курса.\n",
    "\n",
    "---\n",
    "\n",
    "**Предпосылки**\n",
    "\n",
    "Ноутбук предполагает, что:\n",
    "\n",
    "- он находится в той же директории, что и папка `covid_dataset/`;\n",
    "- внутри папки `covid_dataset/metadata/` лежит файл `metadata.parquet`, созданный в ноутбуке `01_preprocess_metadata.ipynb`;\n",
    "- структура каталогов `covid_dataset/images/…` и `covid_dataset/processed/` создана во втором ноутбуке `02_upload_to_hdfs.ipynb`.\n"
   ],
   "id": "11356b578d3e360e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Импорты для обоих режимов: Spark и Pandas\n",
    "import pandas as pd                    # библиотека для табличных данных\n",
    "import os                              # работа с файловой системой\n",
    "\n",
    "# Пытаемся импортировать PySpark\n",
    "try:\n",
    "    from pyspark.sql import SparkSession      # класс для создания Spark-сессии\n",
    "    from pyspark.sql import functions as F    # встроенные функции Spark (SUM, COUNT, окна и т.д.)\n",
    "    from pyspark.sql import Window            # объект для описания оконных функций\n",
    "\n",
    "    PYSPARK_IMPORTED = True                   # флаг: PySpark импортировался\n",
    "except ModuleNotFoundError:\n",
    "    PYSPARK_IMPORTED = False                  # PySpark не установлен в окружении\n",
    "\n",
    "# Флаг, будем ли реально использовать Spark\n",
    "USE_SPARK = False   # по умолчанию считаем, что нет\n",
    "\n",
    "print(\"Пытаемся создать SparkSession...\")\n",
    "\n",
    "if PYSPARK_IMPORTED:\n",
    "    try:\n",
    "        # Пытаемся поднять Spark в локальном режиме (без кластера)\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"Covid19SparkAnalysis\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        print(\"✅ Spark поднялся. Версия:\", spark.version)\n",
    "        USE_SPARK = True                      # отмечаем, что Spark доступен\n",
    "\n",
    "    except Exception as e:\n",
    "        # Если при старте Spark всё равно упали (как у тебя с Java/Hadoop),\n",
    "        # тогда честно переключаемся на Pandas-режим\n",
    "        print(\"⚠️ Spark не смог стартануть в этом окружении.\")\n",
    "        print(\"   Будем работать в режиме Pandas.\")\n",
    "        print(\"   Краткая причина:\", type(e).__name__)\n",
    "        USE_SPARK = False\n",
    "else:\n",
    "    # Если сам PySpark не импортировался\n",
    "    print(\"⚠️ PySpark не установлен. Работаем в режиме Pandas.\")\n",
    "    USE_SPARK = False\n",
    "\n",
    "# Загружаем данные в зависимости от режима\n",
    "METADATA_PARQUET_PATH = \"covid_dataset/metadata/metadata.parquet\"  # путь к parquet-файлу\n",
    "\n",
    "if USE_SPARK:\n",
    "    # Если Spark живой — читаем данные в Spark DataFrame\n",
    "    df_spark = spark.read.parquet(METADATA_PARQUET_PATH)\n",
    "    df_spark.printSchema()\n",
    "    df_spark.show(5, truncate=False)\n",
    "else:\n",
    "    # Если Spark не работает — читаем те же данные в Pandas DataFrame\n",
    "    df_pd = pd.read_parquet(METADATA_PARQUET_PATH)\n",
    "    print(\"Используем режим Pandas. Размер таблицы:\", df_pd.shape)\n",
    "    display(df_pd.head())\n"
   ],
   "id": "39e823165fa5d1d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Распределение укрупнённых диагнозов: COVID19 / PNEUMONIA / NORMAL / OTHER\n",
    "\n",
    "if USE_SPARK:\n",
    "    # --- Ветка Spark SQL ---\n",
    "    df_spark.createOrReplaceTempView(\"covid_metadata\")  # регистрируем временную таблицу\n",
    "\n",
    "    query_diagnosis_distribution = \"\"\"\n",
    "    SELECT\n",
    "        finding_unified,                          -- укрупнённый диагноз\n",
    "        COUNT(*) AS cnt,                          -- число записей\n",
    "        ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS pct  -- доля от общего числа\n",
    "    FROM covid_metadata\n",
    "    GROUP BY finding_unified\n",
    "    ORDER BY cnt DESC\n",
    "    \"\"\"\n",
    "\n",
    "    diagnosis_distribution_df = spark.sql(query_diagnosis_distribution)\n",
    "    print(\"Распределение диагнозов (Spark SQL):\")\n",
    "    diagnosis_distribution_df.show()\n",
    "else:\n",
    "    # --- Ветка Pandas ---\n",
    "    # Считаем, сколько записей в каждом диагнозе\n",
    "    counts = df_pd[\"finding_unified\"].value_counts(dropna=False)        # Series: индекс = диагноз, значение = количество\n",
    "    pct = round(counts / counts.sum() * 100, 2)                         # проценты\n",
    "\n",
    "    diagnosis_distribution_pd = (\n",
    "        pd.DataFrame({\n",
    "            \"finding_unified\": counts.index,    # диагноз\n",
    "            \"cnt\": counts.values,               # количество\n",
    "            \"pct\": pct.values                   # процент\n",
    "        })\n",
    "        .sort_values(\"cnt\", ascending=False)    # сортируем по убыванию количества\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(\"Распределение диагнозов (Pandas):\")\n",
    "    display(diagnosis_distribution_pd)\n"
   ],
   "id": "f815ebfee8a61c6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Количество пациентов по возрастным группам и диагнозам\n",
    "\n",
    "if USE_SPARK:\n",
    "    query_age_diagnosis = \"\"\"\n",
    "    SELECT\n",
    "        age_group,\n",
    "        finding_unified,\n",
    "        COUNT(*) AS cnt\n",
    "    FROM covid_metadata\n",
    "    GROUP BY age_group, finding_unified\n",
    "    ORDER BY age_group, finding_unified\n",
    "    \"\"\"\n",
    "\n",
    "    age_diagnosis_df = spark.sql(query_age_diagnosis)\n",
    "    print(\"Возрастные группы × диагноз (Spark SQL):\")\n",
    "    age_diagnosis_df.show()\n",
    "else:\n",
    "    # В Pandas делаем то же самое через groupby\n",
    "    age_diagnosis_pd = (\n",
    "        df_pd\n",
    "        .groupby([\"age_group\", \"finding_unified\"])    # группируем по возрастной группе и диагнозу\n",
    "        .size()                                       # считаем количество строк\n",
    "        .reset_index(name=\"cnt\")                      # превращаем в колонку cnt\n",
    "        .sort_values([\"age_group\", \"finding_unified\"])\n",
    "    )\n",
    "\n",
    "    print(\"Возрастные группы × диагноз (Pandas):\")\n",
    "    display(age_diagnosis_pd)\n"
   ],
   "id": "d65d44cd7481b848",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ранжирование пациентов по возрасту внутри каждого диагноза\n",
    "\n",
    "if USE_SPARK:\n",
    "    from pyspark.sql import Window\n",
    "\n",
    "    age_rank_window = Window.partitionBy(\"finding_unified\").orderBy(F.col(\"age_filled\").desc())\n",
    "\n",
    "    age_rank_df = (\n",
    "        df_spark\n",
    "        .select(\"patientid\", \"age_filled\", \"age_group\", \"finding_unified\")\n",
    "        .withColumn(\"age_rank_desc\", F.rank().over(age_rank_window))  # RANK() OVER (PARTITION BY ... ORDER BY ...)\n",
    "    )\n",
    "\n",
    "    print(\"Примеры рангов по возрасту (Spark SQL, оконная функция):\")\n",
    "    age_rank_df.orderBy(\"finding_unified\", \"age_rank_desc\").show(20, truncate=False)\n",
    "else:\n",
    "    # В Pandas делаем аналог ранга через groupby + rank()\n",
    "    age_rank_pd = (\n",
    "        df_pd[[\"patientid\", \"age_filled\", \"age_group\", \"finding_unified\"]]\n",
    "        .dropna(subset=[\"age_filled\"])                                  # убираем пустые возраста\n",
    "        .sort_values([\"finding_unified\", \"age_filled\"], ascending=[True, False])\n",
    "    )\n",
    "\n",
    "    # Добавляем колонку rank: ранг возраста по убыванию внутри диагноза\n",
    "    age_rank_pd[\"age_rank_desc\"] = (\n",
    "        age_rank_pd\n",
    "        .groupby(\"finding_unified\")[\"age_filled\"]\n",
    "        .rank(method=\"dense\", ascending=False)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    print(\"Примеры рангов по возрасту (Pandas, аналог оконной функции):\")\n",
    "    display(age_rank_pd.head(20))\n"
   ],
   "id": "99154f3742c6feb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "PARTITIONED_OUTPUT_PATH_SPARK = \"covid_dataset/processed/metadata_partitioned_spark\"\n",
    "PARTITIONED_OUTPUT_PATH_PANDAS = \"covid_dataset/processed/metadata_partitioned_pandas\"\n",
    "\n",
    "if USE_SPARK:\n",
    "    # Запись партиционированного набора Spark'ом\n",
    "    (\n",
    "        df_spark\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"finding_unified\", \"age_group\")\n",
    "        .parquet(PARTITIONED_OUTPUT_PATH_SPARK)\n",
    "    )\n",
    "    print(\"✅ Spark записал партиционированные данные в:\", PARTITIONED_OUTPUT_PATH_SPARK)\n",
    "else:\n",
    "    # Имитация партиционирования через Pandas:\n",
    "    # создаём такую же структуру папок и сохраняем маленькие parquet'ы в каждый каталог\n",
    "    print(\"Spark недоступен. Имитация партиционирования через Pandas...\")\n",
    "\n",
    "    for (diag, age_group), group in df_pd.groupby([\"finding_unified\", \"age_group\"]):\n",
    "        # Формируем путь в стиле Spark: finding_unified=.../age_group=...\n",
    "        subdir = os.path.join(\n",
    "            PARTITIONED_OUTPUT_PATH_PANDAS,\n",
    "            f\"finding_unified={diag}\",\n",
    "            f\"age_group={age_group}\"\n",
    "        )\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "        out_path = os.path.join(subdir, \"part-00000.parquet\")\n",
    "        group.to_parquet(out_path, index=False)\n",
    "\n",
    "    print(\"✅ Pandas записал данные по партициям в:\", PARTITIONED_OUTPUT_PATH_PANDAS)\n"
   ],
   "id": "8068398587421f75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c0439ab9ef64ec7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "408745815d3b00b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8490687af90419c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "25e2b0ff59e210a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "43ac9112fee90254",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f3c6586558f543a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2dfbf62fef2fe125",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "45f741e4b5300330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "861243acef87025d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "67e80f29ebe1efaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8d89a9b36e971724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "468e9632edd7c23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "75eb2bc71f40a714",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
